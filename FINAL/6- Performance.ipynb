{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching\n",
    "\n",
    "\n",
    "\n",
    "First we need to start a SparkSession:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then start the SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# May take a little while on a local computer\n",
    "spark = SparkSession.builder.appName(\"Basics\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You will first need to get the data from a file (or connect to a large distributed file like HDFS, we'll talk about this later once we move to larger datasets on AWS EC2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = spark.createDataFrame(pd.read_csv(\"https://storage.googleapis.com/neurals/data/people.csv\",header='infer'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Showing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note how data is missing!\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance\n",
    "\n",
    "\n",
    "You've been assigned a task that requires running several analysis operations on a DataFrame. You've learned that caching can improve performance when reusing DataFrames and would like to implement it.\n",
    "\n",
    "You'll be working with a new dataset consisting of airline departure information. It may have repetitive data and will need to be de-duplicated.\n",
    "\n",
    "The DataFrame departures_df is defined, but no actions have been performed.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Cache the unique rows in the departures_df DataFrame.\n",
    "- Perform a count query on departures_df, noting how long the operation takes.\n",
    "- Count the rows again, noting the variance in time of a cached DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame from cache\n",
    "\n",
    "You've finished the analysis tasks with the departures_df DataFrame, but have some other processing to do. You'd like to remove the DataFrame from the cache to prevent any excess memory usage on your cluster.\n",
    "\n",
    "The DataFrame departures_df is defined and has already been cached for you.\n",
    "\n",
    "## Instructions\n",
    "- Check the caching status on the departures_df DataFrame.\n",
    "- Remove the departures_df DataFrame from the cache.\n",
    "- Validate the caching status again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if departures_df is in the cache\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)\n",
    "print(\"Removing departures_df from cache\")\n",
    "\n",
    "# Remove departures_df from the cache\n",
    "departures_df.unpersist()\n",
    "\n",
    "# Check the cache status again\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File import performance\n",
    "\n",
    "You've been given a large set of data to import into a Spark DataFrame. You'd like to test the difference in import speed by splitting up the file.\n",
    "\n",
    "You have two types of files available: departures_full.txt.gz and departures_xxx.txt.gz where xxx is 000 - 013. The same number of rows is split between each file.\n",
    "\n",
    "### Instructions\n",
    "- Import the departures_full.txt.gz file and the departures_xxx.txt.gz files into separate DataFrames.\n",
    "- Run a count on each DataFrame and compare the run times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "full_df = spark.createDataFrame(pd.read_csv(\"https://storage.googleapis.com/neurals/data/AA_DFW_2017_Departures_Short.csv.gz\",header='infer'))\n",
    "full_df.show()\n",
    "\n",
    "\n",
    "# Print the count and run time for each DataFrame\n",
    "start_time_a = time.time()\n",
    "print(\"Total rows in full DataFrame:\\t%d\" % full_df.count())\n",
    "print(\"Time to run: %f\" % (time.time() - start_time_a))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Spark configurations\n",
    "\n",
    "You've recently configured a cluster via a cloud provider. Your only access is via the command shell or your python code. You'd like to verify some Spark settings to validate the configuration of the cluster.\n",
    "\n",
    "The spark object is available for use.\n",
    "\n",
    "### Instructions\n",
    "Check the name of the Spark application instance ('spark.app.name').\n",
    "Determine the TCP port the driver runs on ('spark.driver.port').\n",
    "Determine how many partitions are configured for joins.\n",
    "Show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the Spark application instance\n",
    "app_name = spark.conf.get('spark.app.name')\n",
    "\n",
    "# Driver TCP port\n",
    "driver_tcp_port = spark.conf.get('spark.driver.port')\n",
    "\n",
    "# Number of join partitions\n",
    "num_partitions = spark.conf.get('spark.sql.shuffle.partitions')\n",
    "\n",
    "# Show the results\n",
    "print(\"Name: %s\" % app_name)\n",
    "print(\"Driver TCP port: %s\" % driver_tcp_port)\n",
    "print(\"Number of partitions: %s\" % num_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Spark configurations\n",
    "\n",
    "Now that you've reviewed some of the Spark configurations on your cluster, you want to modify some of the settings to tune Spark to your needs. You'll import some data to review that your changes have affected the cluster.\n",
    "\n",
    "The spark configuration is initially set to the default value of 200 partitions.\n",
    "\n",
    "The spark object is available for use. A file named departures.txt.gz is available for import. An initial DataFrame containing the distinct rows from departures.txt.gz is available as departures_df.\n",
    "\n",
    "### Instructions\n",
    "- Store the number of partitions in departures_df in the variable before.\n",
    "- Change the spark.sql.shuffle.partitions configuration to 500 partitions.\n",
    "- Recreate the departures_df DataFrame reading the distinct rows from the departures file.\n",
    "- Print the number of partitions from before and after the configuration change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "departures_df = full_df\n",
    "\n",
    "# Store the number of partitions in variable\n",
    "before = departures_df.rdd.getNumPartitions()\n",
    "\n",
    "# Configure Spark to use 500 partitions\n",
    "spark.conf.set('spark.sql.shuffle.partitions', 500)\n",
    "\n",
    "# Recreate the DataFrame using the departures data file\n",
    "\n",
    "\n",
    "# Print the number of partitions for each instance\n",
    "print(\"Partition count before change: %d\" % before)\n",
    "print(\"Partition count after change: %d\" % departures_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching a DataFrame\n",
    "\n",
    "You've been assigned a task that requires running several analysis operations on a DataFrame. You've learned that caching can improve performance when reusing DataFrames and would like to implement it.\n",
    "\n",
    "You'll be working with a new dataset consisting of airline departure information. It may have repetitive data and will need to be de-duplicated.\n",
    "\n",
    "The DataFrame departures_df is defined, but no actions have been performed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "departures_df = full_df\n",
    "departures_df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if departures_df is in the cache\n",
    "\n",
    "departures_df = full_df\n",
    "departures_df.persist()\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)\n",
    "print(\"Removing departures_df from cache\")\n",
    "\n",
    "# Remove departures_df from the cache\n",
    "departures_df.unpersist()\n",
    "\n",
    "# Check the cache status again\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Spark configurations\n",
    "\n",
    "You've recently configured a cluster via a cloud provider. Your only access is via the command shell or your python code. You'd like to verify some Spark settings to validate the configuration of the cluster.\n",
    "\n",
    "The spark object is available for use.\n",
    "\n",
    "### Instructions\n",
    "Check the name of the Spark application instance ('spark.app.name').\n",
    "Determine the TCP port the driver runs on ('spark.driver.port').\n",
    "Determine how many partitions are configured for joins.\n",
    "Show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the Spark application instance\n",
    "app_name = spark.conf.get('spark.app.name')\n",
    "\n",
    "# Driver TCP port\n",
    "driver_tcp_port = spark.conf.get('spark.driver.port')\n",
    "\n",
    "# Number of join partitions\n",
    "num_partitions = spark.conf.get('spark.sql.shuffle.partitions')\n",
    "\n",
    "# Show the results\n",
    "print(\"Name: %s\" % app_name)\n",
    "print(\"Driver TCP port: %s\" % driver_tcp_port)\n",
    "print(\"Number of partitions: %s\" % num_partitions)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nbpresent": {
   "slides": {},
   "themes": {
    "default": "0535adbc-b74f-46cc-9cd6-4eabe2477c8e",
    "theme": {
     "0535adbc-b74f-46cc-9cd6-4eabe2477c8e": {
      "backgrounds": {
       "backgroundColor": {
        "background-color": "backgroundColor",
        "id": "backgroundColor"
       }
      },
      "id": "0535adbc-b74f-46cc-9cd6-4eabe2477c8e",
      "palette": {
       "backgroundColor": {
        "id": "backgroundColor",
        "rgb": [
         43,
         43,
         43
        ]
       },
       "headingColor": {
        "id": "headingColor",
        "rgb": [
         238,
         238,
         238
        ]
       },
       "linkColor": {
        "id": "linkColor",
        "rgb": [
         19,
         218,
         236
        ]
       },
       "mainColor": {
        "id": "mainColor",
        "rgb": [
         238,
         238,
         238
        ]
       }
      },
      "rules": {
       "a": {
        "color": "linkColor"
       },
       "h1": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 7
       },
       "h2": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 5
       },
       "h3": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 3.75
       },
       "h4": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 3
       },
       "h5": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "h6": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "h7": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "li": {
        "color": "mainColor",
        "font-family": "Lato",
        "font-size": 5
       },
       "p": {
        "color": "mainColor",
        "font-family": "Lato",
        "font-size": 5
       }
      },
      "text-base": {
       "color": "mainColor",
       "font-family": "Lato",
       "font-size": 5
      }
     },
     "cc59980f-cb69-400a-b63a-1fb85ca73c8a": {
      "backgrounds": {
       "dc7afa04-bf90-40b1-82a5-726e3cff5267": {
        "background-color": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "id": "dc7afa04-bf90-40b1-82a5-726e3cff5267"
       }
      },
      "id": "cc59980f-cb69-400a-b63a-1fb85ca73c8a",
      "palette": {
       "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
        "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "rgb": [
         252,
         252,
         252
        ]
       },
       "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
        "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "rgb": [
         68,
         68,
         68
        ]
       },
       "50f92c45-a630-455b-aec3-788680ec7410": {
        "id": "50f92c45-a630-455b-aec3-788680ec7410",
        "rgb": [
         197,
         226,
         245
        ]
       },
       "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
        "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "rgb": [
         43,
         126,
         184
        ]
       },
       "efa7f048-9acb-414c-8b04-a26811511a21": {
        "id": "efa7f048-9acb-414c-8b04-a26811511a21",
        "rgb": [
         25.118061674008803,
         73.60176211453744,
         107.4819383259912
        ]
       }
      },
      "rules": {
       "a": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c"
       },
       "blockquote": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3
       },
       "code": {
        "font-family": "Anonymous Pro"
       },
       "h1": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 8
       },
       "h2": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 6
       },
       "h3": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-family": "Lato",
        "font-size": 5.5
       },
       "h4": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 5
       },
       "h5": {
        "font-family": "Lato"
       },
       "h6": {
        "font-family": "Lato"
       },
       "h7": {
        "font-family": "Lato"
       },
       "li": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3.25
       },
       "pre": {
        "font-family": "Anonymous Pro",
        "font-size": 4
       }
      },
      "text-base": {
       "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
       "font-family": "Lato",
       "font-size": 4
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
